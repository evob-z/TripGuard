import os
from pathlib import Path

from langchain_chroma import Chroma
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings

# --- åŸºç¡€ä¾èµ– ---
# æ—¢ç„¶è‡ªåŠ¨çš„ EnsembleRetriever æ€»æ˜¯æŠ¥é”™ï¼Œæˆ‘ä»¬è¿™é‡Œæ‰‹åŠ¨å®ç°â€œæ£€ç´¢+å»é‡+é‡æ’â€çš„é€»è¾‘
# è¿™éœ€è¦å®‰è£…: pip install rank_bm25 sentence-transformers
os.environ['HF_HUB_OFFLINE'] = '1'
# --- è·¯å¾„é…ç½® ---
CURRENT_FILE_DIR = Path(__file__).parent.resolve()
PERSIST_DIRECTORY = CURRENT_FILE_DIR / "data" / "chroma_db"


def get_manual_hybrid_results(query: str):
    """
    æ‰‹åŠ¨æ‰§è¡Œï¼šå‘é‡æ£€ç´¢ + BM25æ£€ç´¢ -> ç®€å•åˆå¹¶ -> Rerank
    """
    print(f"ğŸ” å¼€å§‹æ‰§è¡Œæ··åˆæ£€ç´¢: {query}")

    # 1. åˆå§‹åŒ– Embedding æ¨¡å‹ (CPU)
    # è¿™ä¸€æ­¥å¦‚æœä¸åŠ  model_kwargs={"device": "cpu"}ï¼Œåœ¨æ— æ˜¾å¡æœºå™¨ä¸Šå¯èƒ½ä¼šæŠ¥é”™
    embedding_model = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={"device": "cpu"},
        encode_kwargs={'normalize_embeddings': True}
    )

    # 2. å‘é‡æ£€ç´¢ (Vector Search) - è¯­ä¹‰å¬å›
    if not PERSIST_DIRECTORY.exists():
        raise FileNotFoundError(f"æ•°æ®åº“æœªæ‰¾åˆ°: {PERSIST_DIRECTORY}")

    vector_db = Chroma(
        persist_directory=str(PERSIST_DIRECTORY),
        embedding_function=embedding_model,
        collection_name="trip_guard_collection"
    )
    # è·å– Top 10
    print("   - æ‰§è¡Œå‘é‡æ£€ç´¢...")
    vector_docs = vector_db.similarity_search(query, k=10)

    # 3. BM25 æ£€ç´¢ (Keyword Search) - å…³é”®è¯å¬å›
    print("   - æ‰§è¡Œå…³é”®è¯æ£€ç´¢...")
    try:
        # è·å–æ‰€æœ‰æ–‡æ¡£ç”¨äºæ„å»ºç´¢å¼•
        all_docs = vector_db.get()['documents']
        all_metadatas = vector_db.get()['metadatas']

        if not all_docs:
            print("   âš ï¸ è­¦å‘Š: æ•°æ®åº“ä¸ºç©ºï¼Œè·³è¿‡ BM25")
            keyword_docs = []
        else:
            bm25_docs = [Document(page_content=t, metadata=m) for t, m in zip(all_docs, all_metadatas)]
            bm25_retriever = BM25Retriever.from_documents(bm25_docs)
            bm25_retriever.k = 10
            keyword_docs = bm25_retriever.invoke(query)
    except Exception as e:
        print(f"   âš ï¸ BM25 æ„å»ºå¤±è´¥(å¯èƒ½æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œæˆ–ä¾èµ–ç¼ºå¤±): {e}")
        keyword_docs = []

    # 4. æ‰‹åŠ¨å»é‡åˆå¹¶ (Ensemble Logic)
    unique_docs = {}
    # å…ˆæ”¾å…¥å‘é‡ç»“æœï¼Œå†æ”¾å…¥å…³é”®è¯ç»“æœ
    for doc in vector_docs + keyword_docs:
        # ä½¿ç”¨å†…å®¹ä½œä¸ºå»é‡é”® (é˜²æ­¢åŒä¸€æ®µè¯è¢«é‡å¤å¬å›)
        key = doc.page_content.strip()
        if key not in unique_docs:
            unique_docs[key] = doc

    merged_docs = list(unique_docs.values())
    print(f"   - å¬å›åˆå¹¶åæ–‡æ¡£æ•°: {len(merged_docs)}")

    if not merged_docs:
        return []

    # 5. æ‰‹åŠ¨é‡æ’åº (Rerank Logic)
    print("   - æ‰§è¡Œé‡æ’åº (Rerank)...")
    # åˆå§‹åŒ–æ‰“åˆ†æ¨¡å‹
    reranker = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-base")

    # æ„é€  Pair: [query, doc_content]
    pairs = [(query, doc.page_content) for doc in merged_docs]

    # ã€æ ¸å¿ƒä¿®å¤ã€‘ä½¿ç”¨ .score() è€Œä¸æ˜¯ .model.predict()
    scores = reranker.score(pairs)

    # å°†åˆ†æ•°ç»‘å®šåˆ°æ–‡æ¡£å¹¶æ’åº
    scored_docs = sorted(
        zip(merged_docs, scores),
        key=lambda x: x[1],
        reverse=True
    )

    # å– Top 3 (ä¸”åˆ†æ•°ä¸èƒ½å¤ªä½ï¼Œæ¯”å¦‚å¤§äº -2)
    final_top_3 = []
    for doc, score in scored_docs[:3]:
        # print(f"      > å¾—åˆ†: {score:.4f} | å†…å®¹: {doc.page_content[:20]}...")
        final_top_3.append(doc)

    return final_top_3


def query_policy(query: str) -> str:
    """
    å¯¹å¤–æ¥å£
    """
    try:
        # ä½¿ç”¨æ‰‹åŠ¨æ··åˆæ£€ç´¢
        docs = get_manual_hybrid_results(query)

        if not docs:
            return "æœªæ‰¾åˆ°ç›¸å…³æ”¿ç­–ä¿¡æ¯ã€‚"

        results = []
        for i, doc in enumerate(docs):
            # è·å– source (build.py ä¸­æˆ‘ä»¬åªå­˜äº†æ–‡ä»¶å)
            source = doc.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")
            content = doc.page_content.strip()

            entry = f"ã€å‚è€ƒèµ„æ–™ {i + 1}ã€‘\næ¥æº: {source}\nå†…å®¹: {content}"
            results.append(entry)

        return "\n\n".join(results)

    except Exception as e:
        import traceback
        traceback.print_exc()  # æ‰“å°å®Œæ•´æŠ¥é”™å †æ ˆ
        return f"æ£€ç´¢ç³»ç»Ÿé”™è¯¯: {str(e)}"


if __name__ == "__main__":
    print("-" * 30)
    print(query_policy("å·®æ—…ä½å®¿æ ‡å‡†"))
