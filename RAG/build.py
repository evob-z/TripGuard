import time
from pathlib import Path

import torch
from dotenv import load_dotenv
# LangChain ç»„ä»¶
from langchain_chroma import Chroma
from langchain_classic.indexes import SQLRecordManager, index
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ================= é…ç½®åŒºåŸŸ =================

# 1. è·¯å¾„é…ç½®
CURRENT_FILE_DIR = Path(__file__).parent.resolve()
DATA_DIR = CURRENT_FILE_DIR / "data"
PERSIST_DIRECTORY = DATA_DIR / "chroma_db"

# 2. ç‹¬ç«‹çš„ SQLite è®°å½•æ•°æ®åº“ (åªæœåŠ¡äº build.py)
# è¿™æ ·å®Œå…¨é¿å¼€äº† MySQL çš„å…¼å®¹æ€§é—®é¢˜ï¼Œä¹Ÿä¸å½±å“ä¸»ç¨‹åºè¿æ¥ MySQL
RECORD_DB_PATH = CURRENT_FILE_DIR / "record_manager_cache.sqlite"
RECORD_MANAGER_DB_URL = f"sqlite:///{RECORD_DB_PATH}"

# 3. ç´¢å¼•å‘½åç©ºé—´ (å›ºå®š ID)
INDEX_NAMESPACE = "trip_guard/policy_v1"

# 4. æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
SUPPORTED_EXTENSIONS = {'.txt', '.pdf'}


# ===========================================

def get_embedding_model():
    """è·å– Embedding æ¨¡å‹å•ä¾‹"""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”Œ è®¾å¤‡çŠ¶æ€: Using {device}")
    return HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )


def load_documents_from_directory(directory: Path):
    """æ‰«æç›®å½•å¹¶åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
    if not directory.exists():
        print(f"âŒ é”™è¯¯: ç›®å½• {directory} ä¸å­˜åœ¨")
        return []

    docs = []
    files = [f for f in directory.iterdir() if f.suffix.lower() in SUPPORTED_EXTENSIONS]

    if not files:
        print("âš ï¸ ç›®å½•ä¸ºç©ºï¼Œæ— éœ€å¤„ç†ã€‚")
        return []

    print(f"ğŸ“‚ æ­£åœ¨æ‰«æç›®å½•: {directory.name} (å…± {len(files)} ä¸ªæ–‡ä»¶)")

    for file_path in files:
        try:
            ext = file_path.suffix.lower()
            if ext == '.txt':
                loader = TextLoader(str(file_path), encoding='utf-8')
            elif ext == '.pdf':
                loader = PyPDFLoader(str(file_path))
            else:
                continue

            file_docs = loader.load()

            # ã€å…ƒæ•°æ®æ ‡å‡†åŒ–ã€‘
            # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºå”¯ä¸€æ ‡è¯† (Source ID)
            for doc in file_docs:
                doc.metadata["source"] = file_path.name
                if "page" in doc.metadata:
                    doc.metadata["source"] += f" (p{doc.metadata['page'] + 1})"

            docs.extend(file_docs)
            print(f"   - âœ… å·²åŠ è½½: {file_path.name}")

        except Exception as e:
            print(f"   - âŒ åŠ è½½å¤±è´¥: {file_path.name} | åŸå› : {e}")

    return docs


def sync_knowledge_base():
    """ä¸»åŒæ­¥é€»è¾‘"""
    print(f"\n{'=' * 40}")
    print(f"ğŸš€ å¼€å§‹åŒæ­¥çŸ¥è¯†åº“ (Mode: SQLite Local)")
    print(f"{'=' * 40}\n")

    # 1. å‡†å¤‡å‘é‡æ•°æ®åº“ (Chroma)
    embedding_model = get_embedding_model()
    vector_db = Chroma(
        persist_directory=str(PERSIST_DIRECTORY),
        embedding_function=embedding_model,
        collection_name="trip_guard_collection"
    )

    # 2. åˆå§‹åŒ–è®°å½•ç®¡ç†å™¨ (ä½¿ç”¨æœ¬åœ° SQLite)
    print(f"ğŸ”— è¿æ¥è®°å½•æ•°æ®åº“: {RECORD_DB_PATH.name}")
    record_manager = SQLRecordManager(
        INDEX_NAMESPACE,
        db_url=RECORD_MANAGER_DB_URL  # <--- å¼ºåˆ¶ä½¿ç”¨æœ¬åœ° SQLite
    )
    record_manager.create_schema()

    # 3. åŠ è½½æ–‡æ¡£
    print("\n1ï¸âƒ£  åŠ è½½æºæ–‡ä»¶...")
    docs = load_documents_from_directory(DATA_DIR)

    if not docs:
        print("   æ²¡æœ‰å¯å¤„ç†çš„æ–‡æ¡£ã€‚")
        return

    # 4. åˆ‡åˆ†æ–‡æ¡£
    print("\n2ï¸âƒ£  æ‰§è¡Œåˆ‡åˆ† (Chunking)...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100,
        separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼", "ï¼Ÿ", " ", ""],
        keep_separator=True
    )
    splits = text_splitter.split_documents(docs)
    print(f"   å…±ç”Ÿæˆ {len(splits)} ä¸ªåˆ‡ç‰‡")

    # 5. æ‰§è¡Œå¢é‡åŒæ­¥
    print("\n3ï¸âƒ£  æ‰§è¡Œæ™ºèƒ½åŒæ­¥ (Indexing)...")

    indexing_start = time.time()
    result = index(
        splits,
        record_manager,
        vector_db,
        cleanup="full",  # ä¿æŒå…¨é‡åŒæ­¥æ¨¡å¼ (æœ¬åœ°åˆ äº†åº“é‡Œä¹Ÿåˆ )
        source_id_key="source"
    )
    indexing_end = time.time()

    print(f"\nğŸ“Š åŒæ­¥æŠ¥å‘Š (è€—æ—¶ {indexing_end - indexing_start:.2f}s):")
    print(f"   ğŸŸ¢ æ–°å¢ (Added):    {result['num_added']}")
    print(f"   ğŸ”µ æ›´æ–° (Updated):  {result['num_updated']}")
    print(f"   âšª è·³è¿‡ (Skipped):  {result['num_skipped']}")
    print(f"   ğŸ”´ åˆ é™¤ (Deleted):  {result['num_deleted']}")
    print(f"\n{'=' * 40}")
    print("âœ… çŸ¥è¯†åº“åŒæ­¥å®Œæˆï¼")


if __name__ == "__main__":
    sync_knowledge_base()
