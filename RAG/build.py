import os
import shutil
from pathlib import Path

from dotenv import load_dotenv
# LangChain ç›¸å…³åº“
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# --- é…ç½®é¡¹ ---
# ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼ŒåŸºäºå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•å®šä½
CURRENT_FILE_DIR = Path(__file__).parent.resolve()
knowledge_base_file = CURRENT_FILE_DIR / "data" / "policy.txt"
persist_directory = CURRENT_FILE_DIR / "data" / "chroma_db"  # æ•°æ®åº“å­˜å‚¨è·¯å¾„

# 1. æ£€æŸ¥æ•°æ®æº
if not knowledge_base_file.exists():
    print(f"âŒ é”™è¯¯: çŸ¥è¯†åº“æ–‡ä»¶ {knowledge_base_file} æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥ data ç›®å½•ã€‚")
    exit()

# 2. æ£€æŸ¥æ—§æ•°æ®åº“
if persist_directory.exists():
    print(f"âš ï¸ æ£€æµ‹åˆ°å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“: {persist_directory}")
    user_input = input("æ˜¯å¦åˆ é™¤æ—§æ•°æ®å¹¶é‡æ–°æ„å»ºï¼Ÿ(y/n): ")
    if user_input.lower() == 'y':
        print("æ­£åœ¨åˆ é™¤æ—§æ•°æ®åº“...")
        shutil.rmtree(persist_directory)  # å¼ºåˆ¶åˆ é™¤æ–‡ä»¶å¤¹
    else:
        print("è·³è¿‡æ„å»ºã€‚")
        exit()

print('--- ğŸš€ å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“ ---')

# 3. åŠ è½½æ–‡æ¡£
print(f'1. æ­£åœ¨åŠ è½½æ–‡ä»¶: {knowledge_base_file}...')
loader = TextLoader(str(knowledge_base_file), encoding='utf-8')
docs = loader.load()

# 4. æ–‡æœ¬åˆ†å‰² (ä½¿ç”¨é€’å½’åˆ†å‰²ï¼Œæ•ˆæœæ›´å¥½)
print('2. æ­£åœ¨è¿›è¡Œæ–‡æœ¬åˆ‡åˆ†...')
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # æ¯ä¸ªå—çš„å¤§å°
    chunk_overlap=50  # é‡å éƒ¨åˆ†ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡ä¸¢å¤±
)
splits = text_splitter.split_documents(docs)
print(f'   - å…±åˆ‡åˆ†ä¸º {len(splits)} ä¸ªç‰‡æ®µ')

# 5. åˆå§‹åŒ– Embedding æ¨¡å‹
print('3. æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹...')

model_name = "BAAI/bge-m3"
print(f"   - (æ³¨æ„) æ­£åœ¨ä¸‹è½½æœ¬åœ°æ¨¡å‹ {model_name}ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
embedding_model = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

# 6. å‘é‡åŒ–å¹¶å­˜å‚¨
print('4. æ­£åœ¨å†™å…¥ ChromaDB (å‘é‡åŒ–)...')
db = Chroma(
    persist_directory=str(persist_directory),  # Chromaéœ€è¦å­—ç¬¦ä¸²è·¯å¾„
    embedding_function=embedding_model
)

BATCH_SIZE = 5000  # å¿…é¡» < 5461

total_docs = len(splits)

for i in range(0, total_docs, BATCH_SIZE):
    # åˆ‡ç‰‡æ“ä½œï¼šå–å‡ºå½“å‰è¿™ä¸€æ‰¹
    batch = splits[i: i + BATCH_SIZE]

    # å†™å…¥å½“å‰æ‰¹æ¬¡
    db.add_documents(batch)

    # æ‰“å°è¿›åº¦
    current_count = min(i + BATCH_SIZE, total_docs)
    print(f"   - å·²æ’å…¥è¿›åº¦: {current_count} / {total_docs}")

print(f'âœ… ç´¢å¼•æ„å»ºå®Œæ¯•ï¼å·²ä¿å­˜åˆ° {persist_directory}')
