"""
TripGuard RAGç³»ç»Ÿ RAGASæ€§èƒ½è¯„ä¼°æµ‹è¯•è„šæœ¬
æµ‹è¯•å„ä¸ªæ£€ç´¢æ¨¡å—çš„ç‹¬ç«‹æ€§èƒ½å’Œç»„åˆæ•ˆæœ
"""
import os
import json
from pathlib import Path
from typing import List, Dict, Any
import time
from datetime import datetime

# å¯¼å…¥RAGç›¸å…³æ¨¡å—
from retriever import (
    get_vector_db, 
    get_embedding_model,
    vector_search,
    bm25_search,
    ensemble_results,
    rerank_documents
)

# å¯¼å…¥LLMç”¨äºåˆ¤æ–­ç›¸å…³æ€§
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
from core.llm import get_llm_model

# å¯¼å…¥RAGASè¯„ä¼°åº“
try:
    from ragas import evaluate
    from ragas.metrics import (
        context_precision,
        context_recall,
        faithfulness,
        answer_relevancy
    )
    from datasets import Dataset
    RAGAS_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: RAGASåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡")
    RAGAS_AVAILABLE = False


# ==================== æ•°æ®åŠ è½½ ====================
def load_test_data(json_path: str) -> List[Dict]:
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


# ==================== æ£€ç´¢æ¨¡å¼å®šä¹‰ ====================
def retrieval_mode_1_vector_only(query: str, k: int = 10) -> List[Any]:
    """æ¨¡å¼1: ä»…å‘é‡æ£€ç´¢"""
    vector_db = get_vector_db()
    return vector_search(query, vector_db, k=k)


def retrieval_mode_2_bm25_only(query: str, k: int = 10) -> List[Any]:
    """æ¨¡å¼2: ä»…BM25å…³é”®è¯æ£€ç´¢"""
    vector_db = get_vector_db()
    return bm25_search(query, vector_db, k=k)


def retrieval_mode_3_hybrid_no_rerank(query: str, k: int = 10) -> List[Any]:
    """æ¨¡å¼3: æ··åˆæ£€ç´¢ï¼ˆå‘é‡+BM25ï¼‰ä½†ä¸é‡æ’åº"""
    vector_db = get_vector_db()
    vector_docs = vector_search(query, vector_db, k=k)
    keyword_docs = bm25_search(query, vector_db, k=k)
    merged_docs = ensemble_results(vector_docs, keyword_docs)
    return merged_docs[:k]  # å–å‰kä¸ª


def retrieval_mode_4_hybrid_with_rerank(query: str, k: int = 10, top_k: int = 3, score_threshold: float = 0.5) -> List[Any]:
    """æ¨¡å¼4: æ··åˆæ£€ç´¢+é‡æ’åºï¼ˆå®Œæ•´pipelineï¼‰
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        k: æ£€ç´¢æ•°é‡
        top_k: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
        score_threshold: é‡æ’åºå¾—åˆ†é˜ˆå€¼ï¼Œä½äºæ­¤å€¼å°†æ‹’è¯†è¿”å›ç©ºåˆ—è¡¨ï¼ˆé»˜è®¤0.5ï¼‰
    """
    vector_db = get_vector_db()
    vector_docs = vector_search(query, vector_db, k=k)
    keyword_docs = bm25_search(query, vector_db, k=k)
    merged_docs = ensemble_results(vector_docs, keyword_docs)
    reranked_docs = rerank_documents(query, merged_docs, top_k=top_k)
    
    # é˜ˆå€¼æœºåˆ¶ï¼šæ£€æŸ¥é‡æ’åºåçš„æœ€é«˜å¾—åˆ†
    if reranked_docs and hasattr(reranked_docs[0], 'metadata'):
        # å‡è®¾rerank_documentsåœ¨metadataä¸­å­˜å‚¨äº†å¾—åˆ†ï¼ˆéœ€è¦æ ¹æ®å®é™…å®ç°è°ƒæ•´ï¼‰
        max_score = reranked_docs[0].metadata.get('rerank_score', 1.0)
        if max_score < score_threshold:
            # æ¨¡æ‹Ÿæ‹’è¯†è¡Œä¸ºï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []
    
    return reranked_docs


# ==================== LLMåˆ¤æ–­å·¥å…· ====================
def get_llm_judge():
    """è·å–LLMåˆ¤æ–­å™¨ï¼Œä½¿ç”¨å¿«é€Ÿqwen-plusæ¨¡å‹"""
    return get_llm_model("intent")  # ä½¿ç”¨å¿«é€Ÿå“åº”çš„qwen-plusæ¨¡å‹


def llm_judge_relevance(question: str, ground_truth: str, retrieved_context: str, llm, is_unanswerable: bool = False) -> bool:
    """ä½¿ç”¨LLMåˆ¤æ–­æ£€ç´¢åˆ°çš„å†…å®¹æ˜¯å¦æ”¯æŒæ ‡å‡†ç­”æ¡ˆ
    
    Args:
        question: æŸ¥è¯¢é—®é¢˜
        ground_truth: æ ‡å‡†ç­”æ¡ˆ
        retrieved_context: æ£€ç´¢åˆ°çš„å†…å®¹
        llm: LLMæ¨¡å‹
        is_unanswerable: æ˜¯å¦ä¸ºæ— ç­”æ¡ˆæŸ¥è¯¢ï¼ˆç”¨äºè°ƒæ•´prompté€»è¾‘ï¼‰
    """
    if is_unanswerable:
        # é’ˆå¯¹æ— ç­”æ¡ˆæŸ¥è¯¢çš„ç‰¹æ®Šprompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯„ä¼°ä¸“å®¶ã€‚è¯·åˆ¤æ–­ã€æ£€ç´¢å†…å®¹ã€‘æ˜¯å¦åŒ…å«å›ç­”ã€é—®é¢˜ã€‘æ‰€éœ€çš„æœ‰æ•ˆä¿¡æ¯ã€‚

ã€é—®é¢˜ã€‘ï¼š{question}

ã€æ£€ç´¢å†…å®¹ã€‘ï¼š{retrieved_context}

è¯·ä»…å›ç­”â€œæ˜¯â€æˆ–â€œå¦â€ã€‚å¦‚æœæ£€ç´¢å†…å®¹åŒ…å«äº†å›ç­”è¯¥é—®é¢˜æ‰€éœ€çš„æœ‰æ•ˆä¿¡æ¯ï¼Œå›ç­”â€œæ˜¯â€ï¼›å¦‚æœæ£€ç´¢å†…å®¹æ— æ³•å›ç­”è¯¥é—®é¢˜æˆ–ä¸ç›¸å…³ï¼Œå›ç­”â€œå¦â€ã€‚

åˆ¤æ–­ï¼š"""
    else:
        # é’ˆå¯¹å¯å›ç­”æŸ¥è¯¢çš„åŸæœ‰prompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯„ä¼°ä¸“å®¶ã€‚è¯·åˆ¤æ–­ã€æ£€ç´¢å†…å®¹ã€‘æ˜¯å¦èƒ½å¤Ÿæ”¯æŒå›ç­”ã€é—®é¢˜ã€‘å¹¶å¾—å‡ºã€æ ‡å‡†ç­”æ¡ˆã€‘ã€‚

ã€é—®é¢˜ã€‘ï¼š{question}

ã€æ ‡å‡†ç­”æ¡ˆã€‘ï¼š{ground_truth}

ã€æ£€ç´¢å†…å®¹ã€‘ï¼š{retrieved_context}

è¯·ä»…å›ç­”â€œæ˜¯â€æˆ–â€œå¦â€ã€‚å¦‚æœæ£€ç´¢å†…å®¹åŒ…å«äº†è¶³å¤Ÿä¿¡æ¯æ¥æ”¯æŒæ ‡å‡†ç­”æ¡ˆï¼Œå›ç­”â€œæ˜¯â€ï¼›å¦åˆ™å›ç­”â€œå¦â€ã€‚

åˆ¤æ–­ï¼š"""
    
    try:
        response = llm.invoke(prompt)
        answer = response.content.strip()
        return "æ˜¯" in answer or "yes" in answer.lower()
    except Exception as e:
        print(f"  âš ï¸ LLMåˆ¤æ–­å¤±è´¥: {e}ï¼Œå›é€€åˆ°å­—ç¬¦ä¸²åŒ¹é…")
        # å›é€€åˆ°ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…
        if is_unanswerable:
            # å¯¹äºæ— ç­”æ¡ˆæŸ¥è¯¢ï¼Œä¿å®ˆåˆ¤æ–­ï¼šå¦‚æŸå†…å®¹å¾ˆå°‘æˆ–ä¸ºç©ºï¼Œåˆ™è®¤ä¸ºä¸ç›¸å…³
            return len(retrieved_context.strip()) > 50
        else:
            return ground_truth in retrieved_context or any(
                keyword in retrieved_context for keyword in ground_truth.split('ï¼Œ')[:3]
            )


# ==================== è¯„ä¼°æŒ‡æ ‡è®¡ç®— ====================
def calculate_recall(retrieved_contexts: List[str], question: str, ground_truth: str, ground_truth_contexts: List[str], llm, k: int = None) -> float:
    """è®¡ç®—å¬å›ç‡:æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å æ‰€æœ‰ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹
    
    Args:
        retrieved_contexts: æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹åˆ—è¡¨
        question: æŸ¥è¯¢é—®é¢˜
        ground_truth: æ ‡å‡†ç­”æ¡ˆ
        ground_truth_contexts: æ ‡å‡†ä¸Šä¸‹æ–‡åˆ—è¡¨
        llm: LLMåˆ¤æ–­å™¨
        k: å¦‚æœæŒ‡å®šï¼Œåªè€ƒè™‘å‰kä¸ªæ£€ç´¢ç»“æœ
    """
    if not ground_truth_contexts:
        return 0.0
    
    # å¦‚æœæŒ‡å®šäº†kï¼Œåªä½¿ç”¨å‰kä¸ªç»“æœ
    contexts_to_check = retrieved_contexts[:k] if k is not None else retrieved_contexts
    
    # ä½¿ç”¨LLMåˆ¤æ–­æ¯ä¸ªæ ‡å‡†ä¸Šä¸‹æ–‡æ˜¯å¦è¢«å¬å›
    matches = 0
    for gt_context in ground_truth_contexts:
        for ret_context in contexts_to_check:
            if llm_judge_relevance(question, ground_truth, ret_context, llm):
                matches += 1
                break
    
    return matches / len(ground_truth_contexts)


def calculate_precision(retrieved_contexts: List[str], question: str, ground_truth: str, llm) -> float:
    """è®¡ç®—ç²¾ç¡®ç‡ï¼šæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹"""
    if not retrieved_contexts:
        return 0.0
    
    # ä½¿ç”¨LLMåˆ¤æ–­æ¯ä¸ªæ£€ç´¢æ–‡æ¡£æ˜¯å¦ç›¸å…³
    matches = 0
    for ret_context in retrieved_contexts:
        if llm_judge_relevance(question, ground_truth, ret_context, llm):
            matches += 1
    
    return matches / len(retrieved_contexts)


def calculate_mrr(retrieved_contexts: List[str], question: str, ground_truth: str, llm) -> float:
    """è®¡ç®—å¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰"""
    for i, context in enumerate(retrieved_contexts):
        if llm_judge_relevance(question, ground_truth, context, llm):
            return 1.0 / (i + 1)
    return 0.0


def calculate_ndcg(retrieved_contexts: List[str], question: str, ground_truth: str, llm) -> float:
    """è®¡ç®—å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Šï¼ˆNDCGï¼‰"""
    import math
    
    if not retrieved_contexts:
        return 0.0
    
    # ä½¿ç”¨LLMåˆ¤æ–­ç›¸å…³æ€§åˆ†æ•°ï¼ˆ1è¡¨ç¤ºç›¸å…³ï¼Œ0è¡¨ç¤ºä¸ç›¸å…³ï¼‰
    relevance_scores = []
    for context in retrieved_contexts:
        is_relevant = llm_judge_relevance(question, ground_truth, context, llm)
        relevance_scores.append(1.0 if is_relevant else 0.0)
    
    # DCG
    if relevance_scores:
        dcg = relevance_scores[0] + sum(
            rel / math.log2(i + 2) for i, rel in enumerate(relevance_scores[1:], 1)
        )
    else:
        dcg = 0.0
    
    # IDCG (ç†æƒ³æƒ…å†µ)
    ideal_scores = sorted(relevance_scores, reverse=True)
    if ideal_scores:
        idcg = ideal_scores[0] + sum(
            rel / math.log2(i + 2) for i, rel in enumerate(ideal_scores[1:], 1)
        )
    else:
        idcg = 0.0
    
    return dcg / idcg if idcg > 0 else 0.0


# ==================== è¯„ä¼°æ‰§è¡Œ ====================
def evaluate_retrieval_mode(
    mode_name: str,
    retrieval_func,
    test_data: List[Dict],
    llm,
    k_values: List[int] = [3, 5, 10],
    warmup: bool = False
) -> Dict[str, Any]:
    """è¯„ä¼°å•ä¸ªæ£€ç´¢æ¨¡å¼ï¼Œæ”¯æŒå¤šä¸ªkå€¼çš„Recallè®¡ç®—
    
    æœ¬å‡½æ•°ä¼šå°†æµ‹è¯•æ•°æ®æ‹†åˆ†ä¸ºä¸¤ç»„ï¼š
    - Answerable: ground_truth_context éç©ºï¼Œä»…è¿™éƒ¨åˆ†ç”¨äºè®¡ç®— Recall/MRR/NDCG
    - Unanswerable: ground_truth_context ä¸ºç©ºï¼Œç”¨äºè®¡ç®—å™ªå£°é²æ£’æ€§ï¼ˆNoise Robustnessï¼‰

    Args:
        mode_name: æ£€ç´¢æ¨¡å¼åç§°
        retrieval_func: æ£€ç´¢å‡½æ•°
        test_data: æµ‹è¯•æ•°æ®
        llm: LLMåˆ¤æ–­å™¨
        k_values: è¦è¯„ä¼°çš„kå€¼åˆ—è¡¨ï¼Œç”¨äºè®¡ç®—Recall@k
        warmup: æ˜¯å¦æ‰§è¡Œé¢„çƒ­
    """
    print(f"\n{'='*60}")
    print(f"å¼€å§‹è¯„ä¼°: {mode_name}")
    print(f"{'='*60}")
    
    # ä½¿ç”¨æœ€å¤§çš„kå€¼è¿›è¡Œæ£€ç´¢
    max_k = max(k_values)
    
    results = {
        'mode_name': mode_name,
        'total_queries': len(test_data),
        'answerable_queries': 0,
        'unanswerable_queries': 0,
        'precision_scores': [],
        'mrr_scores': [],
        'ndcg_scores': [],
        'avg_retrieval_time': 0,
        'failed_queries': 0,
        'noise_robustness_scores': []  # ä»…é’ˆå¯¹æ— ç­”æ¡ˆæŸ¥è¯¢
    }
    
    # ä¸ºæ¯ä¸ªkå€¼åˆ›å»ºç‹¬ç«‹çš„recallåˆ†æ•°åˆ—è¡¨ï¼ˆä»…è®°å½• Answerable æ ·æœ¬ï¼‰
    for k in k_values:
        results[f'recall@{k}_scores'] = []
    
    # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªæ¨¡å¼ï¼Œæ‰§è¡Œä¸€æ¬¡é¢„çƒ­æŸ¥è¯¢
    if warmup:
        print("\nğŸ”¥ æ‰§è¡Œæ¨¡å‹é¢„çƒ­æŸ¥è¯¢...")
        try:
            _ = retrieval_func(test_data[0]['question'], k=max_k)
            print("âœ“ é¢„çƒ­å®Œæˆ\n")
        except:
            pass
    
    total_time = 0
    
    for i, item in enumerate(test_data, 1):
        question = item['question']
        ground_truth = item.get('ground_truth', '')
        ground_truth_contexts = item['ground_truth_context']
        is_answerable = bool(ground_truth_contexts)

        if is_answerable:
            results['answerable_queries'] += 1
        else:
            results['unanswerable_queries'] += 1
        
        print(f"\n[{i}/{len(test_data)}] æŸ¥è¯¢: {question[:50]}... ({'Answerable' if is_answerable else 'Unanswerable'})")
        
        try:
            # æ‰§è¡Œæ£€ç´¢å¹¶è®¡æ—¶ï¼ˆä½¿ç”¨æœ€å¤§kå€¼ï¼‰
            start_time = time.time()
            retrieved_docs = retrieval_func(question, k=max_k)
            elapsed_time = time.time() - start_time
            total_time += elapsed_time
            
            # æå–æ£€ç´¢åˆ°çš„æ–‡æœ¬å†…å®¹
            retrieved_contexts = [doc.page_content for doc in retrieved_docs]
            
            if is_answerable:
                # ä¸ºæ¯ä¸ªkå€¼è®¡ç®— Recall@kï¼ˆä»… Answerable æ ·æœ¬å‚ä¸ï¼‰
                recall_results = {}
                for k in k_values:
                    recall_k = calculate_recall(
                        retrieved_contexts,
                        question,
                        ground_truth,
                        ground_truth_contexts,
                        llm,
                        k=k,
                    )
                    results[f'recall@{k}_scores'].append(recall_k)
                    recall_results[k] = recall_k
                
                # è®¡ç®—å…¶ä»–è¯„ä¼°æŒ‡æ ‡ï¼ˆåŸºäºæœ€å¤§kå€¼çš„ç»“æœï¼Œä»… Answerable æ ·æœ¬ï¼‰
                precision = calculate_precision(retrieved_contexts, question, ground_truth, llm)
                mrr = calculate_mrr(retrieved_contexts, question, ground_truth, llm)
                ndcg = calculate_ndcg(retrieved_contexts, question, ground_truth, llm)
                
                results['precision_scores'].append(precision)
                results['mrr_scores'].append(mrr)
                results['ndcg_scores'].append(ndcg)
                
                # æ‰“å°ç»“æœ
                recall_str = " | ".join([f"Recall@{k}: {recall_results[k]:.3f}" for k in k_values])
                print(f"  âœ“ {recall_str}")
                print(f"    Precision: {precision:.3f} | MRR: {mrr:.3f} | NDCG: {ndcg:.3f} | Time: {elapsed_time:.2f}s")
            else:
                # æ— ç­”æ¡ˆï¼ˆUnanswerableï¼‰æ ·æœ¬ï¼šåªè®¡ç®—å™ªå£°é²æ£’æ€§ï¼ˆTrue Rejectionï¼‰ï¼Œä»…å…³æ³¨ Top-K æ£€ç´¢ç»“æœ
                has_support = False
                noise_k = min(k_values) if k_values else len(retrieved_contexts)
                for ret_context in retrieved_contexts[:noise_k]:
                    if llm_judge_relevance(question, ground_truth, ret_context, llm, is_unanswerable=True):
                        has_support = True
                        break
                is_robust = not has_support
                score = 1.0 if is_robust else 0.0
                results['noise_robustness_scores'].append(score)
                status_str = "âœ…" if is_robust else "âš ï¸"
                print(f"  {status_str} æ— ç­”æ¡ˆæŸ¥è¯¢è¯„ä¼° -> Noise Robustness: {score:.3f} (Top-{noise_k}) | Time: {elapsed_time:.2f}s")
            
        except Exception as e:
            print(f"  âœ— æŸ¥è¯¢å¤±è´¥: {str(e)}")
            results['failed_queries'] += 1
            if is_answerable:
                for k in k_values:
                    results[f'recall@{k}_scores'].append(0.0)
                results['precision_scores'].append(0.0)
                results['mrr_scores'].append(0.0)
                results['ndcg_scores'].append(0.0)
            else:
                # æ— ç­”æ¡ˆæ ·æœ¬å¤±è´¥ï¼Œè§†ä¸ºå™ªå£°é²æ£’æ€§ä¸º0
                results['noise_robustness_scores'].append(0.0)
    
    # è®¡ç®—å¹³å‡å€¼ï¼ˆAnswerable ç»„ï¼‰
    for k in k_values:
        scores = results[f'recall@{k}_scores']
        results[f'avg_recall@{k}'] = sum(scores) / len(scores) if scores else 0.0
    
    if results['precision_scores']:
        results['avg_precision'] = sum(results['precision_scores']) / len(results['precision_scores'])
    else:
        results['avg_precision'] = 0.0
    
    if results['mrr_scores']:
        results['avg_mrr'] = sum(results['mrr_scores']) / len(results['mrr_scores'])
    else:
        results['avg_mrr'] = 0.0
    
    if results['ndcg_scores']:
        results['avg_ndcg'] = sum(results['ndcg_scores']) / len(results['ndcg_scores'])
    else:
        results['avg_ndcg'] = 0.0
    
    # æ— ç­”æ¡ˆç»„çš„å™ªå£°é²æ£’æ€§ï¼ˆTrue Rejection Rateï¼‰
    if results['noise_robustness_scores']:
        results['noise_robustness'] = sum(results['noise_robustness_scores']) / len(results['noise_robustness_scores'])
    else:
        results['noise_robustness'] = 0.0
    
    results['avg_retrieval_time'] = total_time / len(test_data) if test_data else 0.0
    results['success_rate'] = (len(test_data) - results['failed_queries']) / len(test_data) if test_data else 0.0
    
    print(f"\n{'='*60}")
    print(f"{mode_name} è¯„ä¼°å®Œæˆ")
    for k in k_values:
        print(f"å¹³å‡Recall@{k} (Answerable): {results[f'avg_recall@{k}']:.3f}")
    print(f"å¹³å‡ç²¾ç¡®ç‡ (Answerable): {results['avg_precision']:.3f}")
    print(f"å¹³å‡MRR (Answerable): {results['avg_mrr']:.3f}")
    print(f"å¹³å‡NDCG (Answerable): {results['avg_ndcg']:.3f}")
    print(f"æ— ç­”æ¡ˆæŸ¥è¯¢å™ªå£°é²æ£’æ€§ (Noise Robustness): {results['noise_robustness']:.3f}")
    print(f"Answerable æŸ¥è¯¢æ•°: {results['answerable_queries']} / {results['total_queries']}")
    print(f"Unanswerable æŸ¥è¯¢æ•°: {results['unanswerable_queries']} / {results['total_queries']}")
    print(f"å¹³å‡æ£€ç´¢æ—¶é—´: {results['avg_retrieval_time']:.3f}s")
    print(f"æˆåŠŸç‡: {results['success_rate']*100:.1f}%")
    print(f"{'='*60}\n")
    
    return results


# ==================== ç®€åŒ–æŠ¥å‘Šç”Ÿæˆ ====================
def generate_markdown_report(all_results: List[Dict], output_path: str, k_values: List[int] = [3, 5, 10]):
    """ç”ŸæˆåŒ…å«å¤škå€¼Recallçš„Markdownæ ¼å¼æµ‹è¯•æŠ¥å‘Š"""
    
    report = f"""# TripGuard RAGç³»ç»Ÿæ€§èƒ½è¯„ä¼°æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æµ‹è¯•æ•°æ®é‡**: {all_results[0]['total_queries']}ä¸ªæŸ¥è¯¢
**è¯„ä¼°æ–¹æ³•**: ä½¿ç”¨LLMåˆ¤æ–­æ£€ç´¢å†…å®¹æ˜¯å¦æ”¯æŒæ ‡å‡†ç­”æ¡ˆï¼›æŒ‰æ˜¯å¦å­˜åœ¨ ground_truth_context æ‹†åˆ†ä¸ºã€Œå¯å›ç­” (Answerable)ã€ä¸ã€Œæ— ç­”æ¡ˆ (Unanswerable)ã€ä¸¤ç»„
**æ£€ç´¢æ€§èƒ½ (Answerable)**: ä»…åŸºäº ground_truth_context éç©ºçš„æ•°æ®è®¡ç®— Recall@{', Recall@'.join(map(str, k_values))} å’Œ MRR ç­‰æ’åºæŒ‡æ ‡
**æŠ—å¹»è§‰æ€§èƒ½ (Unanswerable)**: åŸºäº ground_truth_context ä¸ºç©ºçš„æ•°æ®è®¡ç®—å™ªå£°é²æ£’æ€§ï¼ˆNoise Robustnessï¼Œè¶Šé«˜è¶Šå¥½ï¼‰
**æ¨¡å¼4é˜ˆå€¼æœºåˆ¶**: æ··åˆæ£€ç´¢+é‡æ’åºæ¨¡å¼å¯ç”¨äº†å¾—åˆ†é˜ˆå€¼æœºåˆ¶ï¼ˆé»˜è®¤0.5ï¼‰ï¼Œå½“é‡æ’åºå¾—åˆ†ä½äºé˜ˆå€¼æ—¶å°†æ‹’è¯†è¿”å›ç©ºç»“æœï¼Œæå‡æŠ—å¹»è§‰æ€§èƒ½

---

## ğŸ“Š æ£€ç´¢æ€§èƒ½ï¼ˆAnswerableï¼‰å¯¹æ¯”

| æ£€ç´¢æ¨¡å¼ | Recall@3 | Recall@5 | Recall@10 | MRR | æ£€ç´¢æ—¶é—´(s) |
|---------|---------|---------|----------|-----|-----------|
"""
    
    for result in all_results:
        recall_str = " | ".join([f"{result.get(f'avg_recall@{k}', 0.0):.3f}" for k in k_values])
        report += f"| {result['mode_name']} | {recall_str} | {result['avg_mrr']:.3f} | {result['avg_retrieval_time']:.3f} |\n"
    
    # æŠ—å¹»è§‰æ€§èƒ½ï¼ˆä»…åŸºäº Unanswerable æ•°æ®ï¼‰
    report += "\n---\n\n## ğŸ›¡ï¸ æŠ—å¹»è§‰æ€§èƒ½ï¼ˆUnanswerableï¼‰\n\n"
    report += "| æ£€ç´¢æ¨¡å¼ | æ— ç­”æ¡ˆæŸ¥è¯¢æ•° | Noise Robustness |\n"
    report += "|---------|--------------|-----------------|\n"
    for result in all_results:
        unanswerable = result.get('unanswerable_queries', 0)
        noise_robustness = result.get('noise_robustness', 0.0)
        report += f"| {result['mode_name']} | {unanswerable} | {noise_robustness:.3f} |\n"
    
    report += "\n---\n\n## ğŸ¯ å…³é”®å‘ç°\n\n"
    
    # ä¸ºæ¯ä¸ªkå€¼æ‰¾å‡ºæœ€ä½³æ¨¡å¼
    report += "### æœ€ä½³æ€§èƒ½æ¨¡å¼\n\n"
    for k in k_values:
        best_mode = max(all_results, key=lambda x: x.get(f'avg_recall@{k}', 0.0))
        report += f"- **æœ€ä½³Recall@{k}**: {best_mode['mode_name']} ({best_mode.get(f'avg_recall@{k}', 0.0):.3f})\n"
    
    best_precision_mode = max(all_results, key=lambda x: x['avg_precision'])
    best_mrr_mode = max(all_results, key=lambda x: x['avg_mrr'])
    best_noise_mode = max(all_results, key=lambda x: x.get('noise_robustness', 0.0))
    
    report += f"""- **æœ€ä½³ç²¾ç¡®ç‡**: {best_precision_mode['mode_name']} ({best_precision_mode['avg_precision']:.3f})
- **æœ€ä½³æ’åº**: {best_mrr_mode['mode_name']} (MRR={best_mrr_mode['avg_mrr']:.3f})
- **æŠ—å¹»è§‰æ€§èƒ½æœ€ä½³**: {best_noise_mode['mode_name']} (Noise Robustness={best_noise_mode.get('noise_robustness', 0.0):.3f})

"""
    
    # æ¨¡å—è´¡çŒ®åº¦åˆ†æï¼ˆåŸºäºå¤šä¸ªkå€¼ï¼‰
    if len(all_results) >= 4:
        vector_only = all_results[0]
        bm25_only = all_results[1]
        hybrid_no_rerank = all_results[2]
        hybrid_with_rerank = all_results[3]
        
        report += "\n---\n\n## ğŸ“ˆ æ¨¡å—è´¡çŒ®åº¦åˆ†æ\n\n"
        
        report += "### BM25æ¨¡å—è´¡çŒ®ï¼ˆæ··åˆæ£€ç´¢ vs çº¯å‘é‡æ£€ç´¢ï¼‰\n\n"
        report += "| æŒ‡æ ‡ | çº¯å‘é‡ | æ··åˆæ£€ç´¢ | æå‡å€¼ | æå‡ç‡ |\n"
        report += "|------|-------|---------|-------|-------|\n"
        
        for k in k_values:
            vector_recall = vector_only.get(f'avg_recall@{k}', 0.0)
            hybrid_recall = hybrid_no_rerank.get(f'avg_recall@{k}', 0.0)
            improvement = hybrid_recall - vector_recall
            improvement_pct = (improvement / vector_recall * 100) if vector_recall > 0 else 0.0
            report += f"| Recall@{k} | {vector_recall:.3f} | {hybrid_recall:.3f} | {improvement:+.3f} | {improvement_pct:+.1f}% |\n"
        
        report += "\n**å…³é”®æ´å¯Ÿ**: BM25æ¨¡å—é€šè¿‡å…³é”®è¯åŒ¹é…è¡¥å……äº†å‘é‡æ£€ç´¢çš„è¯­ä¹‰ç†è§£ï¼Œ"
        if hybrid_no_rerank.get('avg_recall@3', 0.0) > vector_only.get('avg_recall@3', 0.0):
            report += "åœ¨å°kå€¼æ—¶å°¤å…¶æœ‰æ•ˆï¼Œæ˜¾è‘—æå‡äº†å¬å›ç‡ã€‚\n"
        else:
            report += "æ•´ä½“æå‡äº†æ£€ç´¢è¦†ç›–åº¦ã€‚\n"
        
        report += "\n### Rerankæ¨¡å—è´¡çŒ®ï¼ˆå®Œæ•´Pipeline vs æ··åˆæ£€ç´¢ï¼‰\n\n"
        report += "| æŒ‡æ ‡ | æ··åˆæ£€ç´¢ | +Rerank | æå‡å€¼ | æå‡ç‡ |\n"
        report += "|------|---------|---------|-------|-------|\n"
        
        for k in k_values:
            no_rerank_recall = hybrid_no_rerank.get(f'avg_recall@{k}', 0.0)
            with_rerank_recall = hybrid_with_rerank.get(f'avg_recall@{k}', 0.0)
            improvement = with_rerank_recall - no_rerank_recall
            improvement_pct = (improvement / no_rerank_recall * 100) if no_rerank_recall > 0 else 0.0
            report += f"| Recall@{k} | {no_rerank_recall:.3f} | {with_rerank_recall:.3f} | {improvement:+.3f} | {improvement_pct:+.1f}% |\n"
        
        # Rerankå¯¹å…¶ä»–æŒ‡æ ‡çš„å½±å“
        precision_improvement = hybrid_with_rerank['avg_precision'] - hybrid_no_rerank['avg_precision']
        precision_improvement_pct = (precision_improvement / hybrid_no_rerank['avg_precision'] * 100) if hybrid_no_rerank['avg_precision'] > 0 else 0.0
        mrr_improvement = hybrid_with_rerank['avg_mrr'] - hybrid_no_rerank['avg_mrr']
        mrr_improvement_pct = (mrr_improvement / hybrid_no_rerank['avg_mrr'] * 100) if hybrid_no_rerank['avg_mrr'] > 0 else 0.0
        
        report += f"| ç²¾ç¡®ç‡ | {hybrid_no_rerank['avg_precision']:.3f} | {hybrid_with_rerank['avg_precision']:.3f} | {precision_improvement:+.3f} | {precision_improvement_pct:+.1f}% |\n"
        report += f"| MRR | {hybrid_no_rerank['avg_mrr']:.3f} | {hybrid_with_rerank['avg_mrr']:.3f} | {mrr_improvement:+.3f} | {mrr_improvement_pct:+.1f}% |\n"
        
        report += "\n**å…³é”®æ´å¯Ÿ**: Rerankæ¨¡å—é€šè¿‡è¯­ä¹‰ç›¸å…³æ€§é‡æ–°æ’åºï¼Œ"
        if precision_improvement > 0:
            report += "æ˜¾è‘—æå‡äº†ç²¾ç¡®ç‡å’Œæ’åºè´¨é‡ï¼ˆMRRï¼‰ï¼Œ"
        if hybrid_with_rerank.get('avg_recall@3', 0.0) > hybrid_no_rerank.get('avg_recall@3', 0.0):
            report += "å¹¶åœ¨Top-3ç»“æœä¸­æå‡äº†å¬å›ç‡ã€‚\n"
        else:
            report += "ä¼˜åŒ–äº†ç»“æœæ’åºã€‚\n"
        
        # æ£€ç´¢æ·±åº¦åˆ†æ
        report += "\n### æ£€ç´¢æ·±åº¦å½±å“åˆ†æ\n\n"
        report += "ä¸åŒkå€¼ä¸‹çš„æ€§èƒ½å˜åŒ–è¶‹åŠ¿ï¼š\n\n"
        
        for result in all_results:
            report += f"**{result['mode_name']}**:\n"
            recall_values = [result.get(f'avg_recall@{k}', 0.0) for k in k_values]
            for i, k in enumerate(k_values):
                report += f"  - Recall@{k}: {recall_values[i]:.3f}"
                if i > 0:
                    delta = recall_values[i] - recall_values[i-1]
                    report += f" (Î”{delta:+.3f})"
                report += "\n"
            report += "\n"
    
    report += "\n---\n\n## ğŸ“‹ è¯¦ç»†æ•°æ®\n\n"
    for result in all_results:
        report += f"""### {result['mode_name']}

"""
        # æ˜¾ç¤ºæ‰€æœ‰Recall@kæŒ‡æ ‡
        for k in k_values:
            report += f"""- Recall@{k} (Answerable): {result.get(f'avg_recall@{k}', 0.0):.3f}
"""
        report += f"""- MRR (Answerable): {result['avg_mrr']:.3f}
- ç²¾ç¡®ç‡ (Answerable): {result['avg_precision']:.3f}
- NDCG (Answerable): {result['avg_ndcg']:.3f}
- å¯å›ç­”æŸ¥è¯¢æ•°: {result.get('answerable_queries', result['total_queries'])}
- æ— ç­”æ¡ˆæŸ¥è¯¢æ•°: {result.get('unanswerable_queries', 0)}
- å™ªå£°é²æ£’æ€§ (Noise Robustness, æ— ç­”æ¡ˆç»„): {result.get('noise_robustness', 0.0):.3f}
- å¹³å‡æ£€ç´¢æ—¶é—´: {result['avg_retrieval_time']:.3f}ç§’
- æˆåŠŸç‡: {result['success_rate']*100:.1f}%
- å¤±è´¥æŸ¥è¯¢æ•°: {result['failed_queries']}/{result['total_queries']}

"""
    
    report += "\n---\n\n## ğŸ’¡ æ€»ç»“ä¸å»ºè®®\n\n"
    report += "åŸºäºå¤šç»´åº¦Recallè¯„ä¼°çš„ç»“è®ºï¼š\n\n"
    
    # è‡ªåŠ¨ç”Ÿæˆå»ºè®®
    if len(all_results) >= 4:
        best_overall = max(all_results, key=lambda x: sum([x.get(f'avg_recall@{k}', 0.0) for k in k_values]))
        report += f"1. **æ¨èç­–ç•¥**: {best_overall['mode_name']} åœ¨ç»¼åˆæ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³\n"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„kå€¼æ•æ„Ÿæ€§
        mode_4_recalls = [hybrid_with_rerank.get(f'avg_recall@{k}', 0.0) for k in k_values]
        if max(mode_4_recalls) - min(mode_4_recalls) > 0.1:
            report += f"2. **æ£€ç´¢æ·±åº¦**: ä¸åŒkå€¼ä¸‹æ€§èƒ½å·®å¼‚æ˜æ˜¾ï¼Œå»ºè®®æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„Top-Kå€¼\n"
        else:
            report += f"2. **æ£€ç´¢æ·±åº¦**: å„kå€¼ä¸‹æ€§èƒ½ç¨³å®šï¼Œç³»ç»Ÿé²æ£’æ€§è‰¯å¥½\n"
        
        if precision_improvement > 0.05:
            report += f"3. **é‡æ’åºä»·å€¼**: Rerankæ¨¡å—å¸¦æ¥æ˜¾è‘—æå‡ï¼ˆç²¾ç¡®ç‡+{precision_improvement:.1%}ï¼‰ï¼Œå»ºè®®ä¿ç•™\n"
        
        if any(hybrid_no_rerank.get(f'avg_recall@{k}', 0.0) > vector_only.get(f'avg_recall@{k}', 0.0) * 1.1 for k in k_values):
            report += f"4. **æ··åˆæ£€ç´¢ä¼˜åŠ¿**: BM25+å‘é‡æ··åˆç­–ç•¥ç›¸æ¯”å•ä¸€æ–¹æ³•æœ‰æ˜æ˜¾ä¼˜åŠ¿\n"
    
    # å†™å…¥æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nâœ… æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
    print(f"\næŠ¥å‘Šé¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰:\n{report[:500]}...")


# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("=" * 80)
    print("TripGuard RAGç³»ç»Ÿ å¤šç»´åº¦Recallæ€§èƒ½è¯„ä¼°æµ‹è¯•")
    print("=" * 80)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data_path = Path(__file__).parent / "test_data.json"
    test_data = load_test_data(test_data_path)
    print(f"\nâœ“ å·²åŠ è½½ {len(test_data)} æ¡æµ‹è¯•æ•°æ®")
    
    # åˆå§‹åŒ–LLMåˆ¤æ–­å™¨
    print("\nâœ“ æ­£åœ¨åˆå§‹åŒ–LLMåˆ¤æ–­å™¨...")
    llm = get_llm_judge()
    print("âœ“ LLMåˆ¤æ–­å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # å®šä¹‰è¦è¯„ä¼°çš„kå€¼
    k_values = [3, 5, 10]
    print(f"\nâœ“ å°†è¯„ä¼°ä»¥ä¸‹kå€¼çš„RecallæŒ‡æ ‡: {k_values}")
    
    # å®šä¹‰æ‰€æœ‰æµ‹è¯•æ¨¡å¼
    test_modes = [
        ("æ¨¡å¼1: çº¯å‘é‡æ£€ç´¢", retrieval_mode_1_vector_only),
        ("æ¨¡å¼2: çº¯BM25å…³é”®è¯æ£€ç´¢", retrieval_mode_2_bm25_only),
        ("æ¨¡å¼3: æ··åˆæ£€ç´¢ï¼ˆæ— é‡æ’åºï¼‰", retrieval_mode_3_hybrid_no_rerank),
        ("æ¨¡å¼4: æ··åˆæ£€ç´¢+é‡æ’åºï¼ˆå®Œæ•´Pipelineï¼‰", retrieval_mode_4_hybrid_with_rerank),
    ]
    
    # æ‰§è¡Œè¯„ä¼°
    all_results = []
    for i, (mode_name, retrieval_func) in enumerate(test_modes):
        # ç¬¬ä¸€ä¸ªæ¨¡å¼éœ€è¦é¢„çƒ­
        result = evaluate_retrieval_mode(
            mode_name, 
            retrieval_func, 
            test_data, 
            llm,
            k_values=k_values,
            warmup=(i == 0)
        )
        all_results.append(result)
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = Path(__file__).parent / f"RAGAS_Test_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    generate_markdown_report(all_results, report_path, k_values=k_values)
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“Š è¯„ä¼°äº† {len(k_values)} ä¸ªä¸åŒkå€¼çš„RecallæŒ‡æ ‡")
    print("=" * 80)


if __name__ == "__main__":
    main()
