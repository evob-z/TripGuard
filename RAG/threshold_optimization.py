"""
TripGuard RAGç³»ç»Ÿ Reranké˜ˆå€¼ä¼˜åŒ–æµ‹è¯•è„šæœ¬
é€šè¿‡æµ‹è¯•ä¸åŒé˜ˆå€¼æ¥æ‰¾åˆ°å¬å›ç‡å’ŒæŠ—å¹»è§‰æ€§èƒ½çš„æœ€ä½³å¹³è¡¡ç‚¹
"""
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple

import matplotlib
# å¯¼å…¥matplotlibç”¨äºå¯è§†åŒ–
import matplotlib.pyplot as plt

matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
matplotlib.rcParams['axes.unicode_minus'] = False

# ä»test_ragas.pyå¯¼å…¥æ‰€æœ‰éœ€è¦çš„å‡½æ•°
from test_ragas import (
    load_test_data,
    get_llm_judge,
    llm_judge_relevance,
    calculate_recall,
    retrieval_mode_4_hybrid_with_rerank
)

# å¯¼å…¥RAGæ¨¡å—
from retriever import get_vector_db, vector_search, bm25_search, ensemble_results, rerank_documents


# ==================== é˜ˆå€¼ä¼˜åŒ–æµ‹è¯• ====================
def evaluate_threshold(
    threshold: float,
    test_data: List[Dict],
    llm,
    k: int = 10,
    top_k: int = 3,
    k_values: List[int] = [3, 5, 10]
) -> Dict[str, Any]:
    """è¯„ä¼°å•ä¸ªé˜ˆå€¼çš„æ€§èƒ½
    
    Args:
        threshold: è¦æµ‹è¯•çš„é˜ˆå€¼
        test_data: æµ‹è¯•æ•°æ®
        llm: LLMåˆ¤æ–­å™¨
        k: æ£€ç´¢æ•°é‡
        top_k: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
        k_values: è¦è¯„ä¼°çš„kå€¼åˆ—è¡¨
    
    Returns:
        åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•é˜ˆå€¼: {threshold:.2f}")
    print(f"{'='*60}")
    
    # æœ€å¤§kå€¼ç”¨äºæ£€ç´¢
    max_k = max(k_values)
    
    results = {
        'threshold': threshold,
        'total_queries': len(test_data),
        'answerable_queries': 0,
        'unanswerable_queries': 0,
        'rejected_queries': 0,  # å› é˜ˆå€¼è¿‡æ»¤è¢«æ‹’ç»çš„æŸ¥è¯¢æ•°
        'avg_retrieval_time': 0,
        'failed_queries': 0,
        'noise_robustness_scores': []
    }
    
    # ä¸ºæ¯ä¸ªkå€¼åˆ›å»ºç‹¬ç«‹çš„recallåˆ†æ•°åˆ—è¡¨
    for k_val in k_values:
        results[f'recall@{k_val}_scores'] = []
    
    total_time = 0
    
    for i, item in enumerate(test_data, 1):
        question = item['question']
        ground_truth = item.get('ground_truth', '')
        ground_truth_contexts = item['ground_truth_context']
        is_answerable = bool(ground_truth_contexts)
        
        if is_answerable:
            results['answerable_queries'] += 1
        else:
            results['unanswerable_queries'] += 1
        
        try:
            # æ‰§è¡Œæ£€ç´¢å¹¶è®¡æ—¶
            start_time = time.time()
            
            # æ‰‹åŠ¨æ‰§è¡Œæ··åˆæ£€ç´¢+é‡æ’åºæµç¨‹ä»¥è·å–å¾—åˆ†ä¿¡æ¯
            vector_db = get_vector_db()
            vector_docs = vector_search(question, vector_db, k=k)
            keyword_docs = bm25_search(question, vector_db, k=k)
            merged_docs = ensemble_results(vector_docs, keyword_docs)
            reranked_docs = rerank_documents(question, merged_docs, top_k=top_k)
            
            # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
            # æ³¨æ„ï¼šrerank_documentså¯èƒ½åœ¨metadataä¸­å­˜å‚¨å¾—åˆ†
            # è¿™é‡Œå‡è®¾ç¬¬ä¸€ä¸ªæ–‡æ¡£å¾—åˆ†æœ€é«˜ï¼Œå¦‚æœå¾—åˆ†ä½äºé˜ˆå€¼åˆ™æ‹’ç»
            if reranked_docs and hasattr(reranked_docs[0], 'metadata'):
                max_score = reranked_docs[0].metadata.get('rerank_score', 1.0)
                if max_score < threshold:
                    reranked_docs = []  # æ‹’ç»è¿”å›ç»“æœ
                    results['rejected_queries'] += 1
            
            elapsed_time = time.time() - start_time
            total_time += elapsed_time
            
            # æå–æ£€ç´¢åˆ°çš„æ–‡æœ¬å†…å®¹
            retrieved_contexts = [doc.page_content for doc in reranked_docs]
            
            if is_answerable:
                # ä¸ºæ¯ä¸ªkå€¼è®¡ç®— Recall@k
                for k_val in k_values:
                    recall_k = calculate_recall(
                        retrieved_contexts,
                        question,
                        ground_truth,
                        ground_truth_contexts,
                        llm,
                        k=k_val
                    )
                    results[f'recall@{k_val}_scores'].append(recall_k)
                
                if i % 5 == 0:  # æ¯5ä¸ªæŸ¥è¯¢æ‰“å°ä¸€æ¬¡è¿›åº¦
                    print(f"  [{i}/{len(test_data)}] AnswerableæŸ¥è¯¢è¯„ä¼°ä¸­...")
            else:
                # æ— ç­”æ¡ˆæŸ¥è¯¢ï¼šè®¡ç®—å™ªå£°é²æ£’æ€§
                has_support = False
                noise_k = min(k_values) if k_values else len(retrieved_contexts)
                for ret_context in retrieved_contexts[:noise_k]:
                    if llm_judge_relevance(question, ground_truth, ret_context, llm, is_unanswerable=True):
                        has_support = True
                        break
                is_robust = not has_support
                score = 1.0 if is_robust else 0.0
                results['noise_robustness_scores'].append(score)
                
                if i % 5 == 0:
                    print(f"  [{i}/{len(test_data)}] UnanswerableæŸ¥è¯¢è¯„ä¼°ä¸­...")
        
        except Exception as e:
            print(f"  âœ— æŸ¥è¯¢å¤±è´¥: {str(e)}")
            results['failed_queries'] += 1
            if is_answerable:
                for k_val in k_values:
                    results[f'recall@{k_val}_scores'].append(0.0)
            else:
                results['noise_robustness_scores'].append(0.0)
    
    # è®¡ç®—å¹³å‡å€¼
    for k_val in k_values:
        scores = results[f'recall@{k_val}_scores']
        results[f'avg_recall@{k_val}'] = sum(scores) / len(scores) if scores else 0.0
    
    if results['noise_robustness_scores']:
        results['noise_robustness'] = sum(results['noise_robustness_scores']) / len(results['noise_robustness_scores'])
    else:
        results['noise_robustness'] = 0.0
    
    results['avg_retrieval_time'] = total_time / len(test_data) if test_data else 0.0
    results['rejection_rate'] = results['rejected_queries'] / len(test_data) if test_data else 0.0
    
    print(f"\né˜ˆå€¼ {threshold:.2f} è¯„ä¼°å®Œæˆ:")
    for k_val in k_values:
        print(f"  Recall@{k_val}: {results[f'avg_recall@{k_val}']:.3f}")
    print(f"  å™ªå£°é²æ£’æ€§: {results['noise_robustness']:.3f}")
    print(f"  æ‹’ç»ç‡: {results['rejection_rate']*100:.1f}%")
    print(f"{'='*60}")
    
    return results


def find_optimal_threshold(
    thresholds: List[float],
    test_data: List[Dict],
    llm,
    k_values: List[int] = [3, 5, 10],
    alpha: float = 0.6
) -> tuple[Any, list[dict[str, Any]]]:
    """å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
    
    Args:
        thresholds: è¦æµ‹è¯•çš„é˜ˆå€¼åˆ—è¡¨
        test_data: æµ‹è¯•æ•°æ®
        llm: LLMåˆ¤æ–­å™¨
        k_values: è¦è¯„ä¼°çš„kå€¼åˆ—è¡¨
        alpha: å¬å›ç‡æƒé‡ï¼ˆ1-alphaä¸ºæŠ—å¹»è§‰æƒé‡ï¼‰
    
    Returns:
        (æœ€ä¼˜é˜ˆå€¼, æ‰€æœ‰ç»“æœåˆ—è¡¨)
    """
    print("\n" + "="*80)
    print("å¼€å§‹é˜ˆå€¼ä¼˜åŒ–æµ‹è¯•")
    print(f"æµ‹è¯•é˜ˆå€¼èŒƒå›´: {min(thresholds):.1f} ~ {max(thresholds):.1f}")
    print(f"è¯„ä¼°æƒé‡: å¬å›ç‡={alpha:.1%}, æŠ—å¹»è§‰={1-alpha:.1%}")
    print("="*80)
    
    all_results = []
    
    # é¢„çƒ­
    print("\nğŸ”¥ æ‰§è¡Œæ¨¡å‹é¢„çƒ­...")
    try:
        _ = retrieval_mode_4_hybrid_with_rerank(test_data[0]['question'], k=10, top_k=3, score_threshold=0.5)
        print("âœ“ é¢„çƒ­å®Œæˆ\n")
    except:
        pass
    
    # æµ‹è¯•æ¯ä¸ªé˜ˆå€¼
    for threshold in thresholds:
        result = evaluate_threshold(
            threshold,
            test_data,
            llm,
            k=10,
            top_k=3,
            k_values=k_values
        )
        all_results.append(result)
        time.sleep(1)  # é¿å…APIé™æµ
    
    # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆä½¿ç”¨Recall@3ä½œä¸ºä»£è¡¨æ€§æŒ‡æ ‡ï¼‰
    for result in all_results:
        recall_score = result['avg_recall@3']
        noise_score = result['noise_robustness']
        result['combined_score'] = alpha * recall_score + (1 - alpha) * noise_score
    
    # æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼
    best_result = max(all_results, key=lambda x: x['combined_score'])
    optimal_threshold = best_result['threshold']
    
    print(f"\nğŸ¯ æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.2f}")
    print(f"   Recall@3: {best_result['avg_recall@3']:.3f}")
    print(f"   å™ªå£°é²æ£’æ€§: {best_result['noise_robustness']:.3f}")
    print(f"   ç»¼åˆå¾—åˆ†: {best_result['combined_score']:.3f}")
    
    return optimal_threshold, all_results


# ==================== å¯è§†åŒ– ====================
def plot_threshold_analysis(all_results: List[Dict], k_values: List[int], output_dir: Path, alpha: float = 0.6):
    """ç”Ÿæˆé˜ˆå€¼åˆ†æçš„å¯è§†åŒ–å›¾è¡¨
    
    Args:
        all_results: æ‰€æœ‰é˜ˆå€¼çš„è¯„ä¼°ç»“æœ
        k_values: è¯„ä¼°çš„kå€¼åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        alpha: å¬å›ç‡æƒé‡
    """
    thresholds = [r['threshold'] for r in all_results]
    
    # å›¾1: é˜ˆå€¼ vs å¤šä¸ªRecall@k
    fig1, ax1 = plt.subplots(figsize=(10, 6))
    
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
    for i, k_val in enumerate(k_values):
        recall_scores = [r[f'avg_recall@{k_val}'] for r in all_results]
        ax1.plot(thresholds, recall_scores, marker='o', linewidth=2, 
                label=f'Recall@{k_val}', color=colors[i % len(colors)])
    
    ax1.set_xlabel('é‡æ’åºé˜ˆå€¼', fontsize=12)
    ax1.set_ylabel('å¬å›ç‡ (Recall)', fontsize=12)
    ax1.set_title('é‡æ’åºé˜ˆå€¼å¯¹å¬å›ç‡çš„å½±å“', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.legend(loc='best', fontsize=10)
    
    plot1_path = output_dir / "threshold_vs_recall.png"
    plt.tight_layout()
    plt.savefig(plot1_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ å¬å›ç‡æ›²çº¿å›¾å·²ä¿å­˜: {plot1_path}")
    plt.close()
    
    # å›¾2: é˜ˆå€¼ vs å™ªå£°é²æ£’æ€§
    fig2, ax2 = plt.subplots(figsize=(10, 6))
    
    noise_scores = [r['noise_robustness'] for r in all_results]
    ax2.plot(thresholds, noise_scores, marker='s', linewidth=2, 
            color='#d62728', label='å™ªå£°é²æ£’æ€§ (Noise Robustness)')
    
    ax2.set_xlabel('é‡æ’åºé˜ˆå€¼', fontsize=12)
    ax2.set_ylabel('å™ªå£°é²æ£’æ€§', fontsize=12)
    ax2.set_title('é‡æ’åºé˜ˆå€¼å¯¹æŠ—å¹»è§‰æ€§èƒ½çš„å½±å“', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.legend(loc='best', fontsize=10)
    
    plot2_path = output_dir / "threshold_vs_noise_robustness.png"
    plt.tight_layout()
    plt.savefig(plot2_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ æŠ—å¹»è§‰æ€§èƒ½æ›²çº¿å›¾å·²ä¿å­˜: {plot2_path}")
    plt.close()
    
    # å›¾3: ç»¼åˆè§†å›¾ï¼ˆåŒè½´å›¾ï¼‰
    fig3, ax3 = plt.subplots(figsize=(12, 6))
    
    # å·¦è½´ï¼šRecall@3
    recall3_scores = [r['avg_recall@3'] for r in all_results]
    line1 = ax3.plot(thresholds, recall3_scores, marker='o', linewidth=2, 
                     color='#1f77b4', label='Recall@3')
    ax3.set_xlabel('é‡æ’åºé˜ˆå€¼', fontsize=12)
    ax3.set_ylabel('å¬å›ç‡ (Recall@3)', fontsize=12, color='#1f77b4')
    ax3.tick_params(axis='y', labelcolor='#1f77b4')
    
    # å³è½´ï¼šå™ªå£°é²æ£’æ€§
    ax3_right = ax3.twinx()
    line2 = ax3_right.plot(thresholds, noise_scores, marker='s', linewidth=2, 
                          color='#d62728', label='å™ªå£°é²æ£’æ€§')
    ax3_right.set_ylabel('å™ªå£°é²æ£’æ€§ (Noise Robustness)', fontsize=12, color='#d62728')
    ax3_right.tick_params(axis='y', labelcolor='#d62728')
    
    # æ ‡è®°æœ€ä¼˜ç‚¹
    best_result = max(all_results, key=lambda x: x['combined_score'])
    best_threshold = best_result['threshold']
    best_recall3 = best_result['avg_recall@3']
    best_noise = best_result['noise_robustness']
    
    ax3.axvline(x=best_threshold, color='green', linestyle='--', linewidth=1.5, alpha=0.7)
    ax3.plot(best_threshold, best_recall3, 'g*', markersize=15, 
            label=f'æœ€ä¼˜é˜ˆå€¼ ({best_threshold:.2f})')
    
    # åˆå¹¶å›¾ä¾‹
    lines1, labels1 = ax3.get_legend_handles_labels()
    lines2, labels2 = ax3_right.get_legend_handles_labels()
    ax3.legend(lines1 + lines2, labels1 + labels2, loc='upper center', fontsize=10)
    
    ax3.set_title(f'å¬å›ç‡ä¸æŠ—å¹»è§‰æ€§èƒ½çš„æƒè¡¡ (æƒé‡: {alpha:.0%} vs {1-alpha:.0%})', 
                 fontsize=14, fontweight='bold')
    ax3.grid(True, alpha=0.3)
    
    plot3_path = output_dir / "threshold_tradeoff.png"
    plt.tight_layout()
    plt.savefig(plot3_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ æƒè¡¡åˆ†æå›¾å·²ä¿å­˜: {plot3_path}")
    plt.close()
    
    # å›¾4: ç»¼åˆå¾—åˆ†æ›²çº¿
    fig4, ax4 = plt.subplots(figsize=(10, 6))
    
    combined_scores = [r['combined_score'] for r in all_results]
    ax4.plot(thresholds, combined_scores, marker='D', linewidth=2, 
            color='#9467bd', label=f'ç»¼åˆå¾—åˆ† (Î±={alpha:.1f})')
    
    # æ ‡è®°æœ€ä¼˜ç‚¹
    ax4.axvline(x=best_threshold, color='green', linestyle='--', linewidth=1.5, alpha=0.7)
    ax4.plot(best_threshold, best_result['combined_score'], 'g*', markersize=15,
            label=f'æœ€ä¼˜é˜ˆå€¼ ({best_threshold:.2f})')
    
    ax4.set_xlabel('é‡æ’åºé˜ˆå€¼', fontsize=12)
    ax4.set_ylabel('ç»¼åˆå¾—åˆ†', fontsize=12)
    ax4.set_title(f'åŠ æƒç»¼åˆå¾—åˆ† (Recallæƒé‡={alpha:.0%}, æŠ—å¹»è§‰æƒé‡={1-alpha:.0%})', 
                 fontsize=14, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    ax4.legend(loc='best', fontsize=10)
    
    plot4_path = output_dir / "threshold_combined_score.png"
    plt.tight_layout()
    plt.savefig(plot4_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ ç»¼åˆå¾—åˆ†æ›²çº¿å›¾å·²ä¿å­˜: {plot4_path}")
    plt.close()
    
    # å›¾5: æ‹’ç»ç‡æ›²çº¿
    fig5, ax5 = plt.subplots(figsize=(10, 6))
    
    rejection_rates = [r['rejection_rate'] * 100 for r in all_results]
    ax5.plot(thresholds, rejection_rates, marker='^', linewidth=2, 
            color='#8c564b', label='æŸ¥è¯¢æ‹’ç»ç‡')
    
    ax5.set_xlabel('é‡æ’åºé˜ˆå€¼', fontsize=12)
    ax5.set_ylabel('æ‹’ç»ç‡ (%)', fontsize=12)
    ax5.set_title('é‡æ’åºé˜ˆå€¼å¯¹æŸ¥è¯¢æ‹’ç»ç‡çš„å½±å“', fontsize=14, fontweight='bold')
    ax5.grid(True, alpha=0.3)
    ax5.legend(loc='best', fontsize=10)
    
    plot5_path = output_dir / "threshold_vs_rejection_rate.png"
    plt.tight_layout()
    plt.savefig(plot5_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ æ‹’ç»ç‡æ›²çº¿å›¾å·²ä¿å­˜: {plot5_path}")
    plt.close()


def generate_optimization_report(all_results: List[Dict], k_values: List[int], 
                                 output_path: Path, alpha: float = 0.6):
    """ç”Ÿæˆé˜ˆå€¼ä¼˜åŒ–çš„MarkdownæŠ¥å‘Š
    
    Args:
        all_results: æ‰€æœ‰é˜ˆå€¼çš„è¯„ä¼°ç»“æœ
        k_values: è¯„ä¼°çš„kå€¼åˆ—è¡¨
        output_path: è¾“å‡ºè·¯å¾„
        alpha: å¬å›ç‡æƒé‡
    """
    best_result = max(all_results, key=lambda x: x['combined_score'])
    optimal_threshold = best_result['threshold']
    
    report = f"""# TripGuard RAGç³»ç»Ÿ Reranké˜ˆå€¼ä¼˜åŒ–æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æµ‹è¯•æ•°æ®é‡**: {all_results[0]['total_queries']}ä¸ªæŸ¥è¯¢
**æµ‹è¯•é˜ˆå€¼èŒƒå›´**: {min(r['threshold'] for r in all_results):.2f} ~ {max(r['threshold'] for r in all_results):.2f}
**è¯„ä¼°æƒé‡**: å¬å›ç‡={alpha:.0%}, æŠ—å¹»è§‰æ€§èƒ½={1-alpha:.0%}
**è¯„ä¼°æŒ‡æ ‡**: Recall@{', Recall@'.join(map(str, k_values))}, å™ªå£°é²æ£’æ€§ (Noise Robustness)

---

## ğŸ¯ æœ€ä¼˜é˜ˆå€¼å»ºè®®

åŸºäºåŠ æƒç»¼åˆå¾—åˆ†åˆ†æï¼Œ**æ¨èä½¿ç”¨é˜ˆå€¼: {optimal_threshold:.2f}**

### æ€§èƒ½è¡¨ç°

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
"""
    
    for k_val in k_values:
        report += f"| Recall@{k_val} | {best_result[f'avg_recall@{k_val}']:.3f} |\n"
    
    report += f"""| å™ªå£°é²æ£’æ€§ | {best_result['noise_robustness']:.3f} |
| ç»¼åˆå¾—åˆ† | {best_result['combined_score']:.3f} |
| æ‹’ç»ç‡ | {best_result['rejection_rate']*100:.1f}% |
| å¹³å‡æ£€ç´¢æ—¶é—´ | {best_result['avg_retrieval_time']:.3f}ç§’ |

---

## ğŸ“Š å…¨é‡é˜ˆå€¼å¯¹æ¯”

### å¬å›ç‡å¯¹æ¯”

| é˜ˆå€¼ | Recall@3 | Recall@5 | Recall@10 | ç»¼åˆå¾—åˆ† |
|------|----------|----------|-----------|---------|
"""
    
    for result in all_results:
        recall_str = " | ".join([f"{result[f'avg_recall@{k_val}']:.3f}" for k_val in k_values])
        report += f"| {result['threshold']:.2f} | {recall_str} | {result['combined_score']:.3f} |\n"
    
    report += "\n### æŠ—å¹»è§‰æ€§èƒ½å¯¹æ¯”\n\n"
    report += "| é˜ˆå€¼ | å™ªå£°é²æ£’æ€§ | æ‹’ç»ç‡ |\n"
    report += "|------|-----------|-------|\n"
    
    for result in all_results:
        report += f"| {result['threshold']:.2f} | {result['noise_robustness']:.3f} | {result['rejection_rate']*100:.1f}% |\n"
    
    report += "\n---\n\n## ğŸ“ˆ å…³é”®æ´å¯Ÿ\n\n"
    
    # åˆ†æè¶‹åŠ¿
    thresholds = [r['threshold'] for r in all_results]
    recall3_scores = [r['avg_recall@3'] for r in all_results]
    noise_scores = [r['noise_robustness'] for r in all_results]
    
    # æ‰¾å‡ºå¬å›ç‡å’ŒæŠ—å¹»è§‰æ€§èƒ½çš„æ‹ç‚¹
    max_recall3_idx = recall3_scores.index(max(recall3_scores))
    max_noise_idx = noise_scores.index(max(noise_scores))
    
    report += f"""### 1. å¬å›ç‡è¶‹åŠ¿

- **æœ€é«˜å¬å›ç‡**: é˜ˆå€¼={thresholds[max_recall3_idx]:.2f}æ—¶, Recall@3={recall3_scores[max_recall3_idx]:.3f}
- **è¶‹åŠ¿åˆ†æ**: {"é˜ˆå€¼è¶Šä½ï¼Œå¬å›ç‡è¶Šé«˜" if recall3_scores[0] > recall3_scores[-1] else "é˜ˆå€¼è¶Šé«˜ï¼Œå¬å›ç‡è¶Šé«˜"}

### 2. æŠ—å¹»è§‰æ€§èƒ½è¶‹åŠ¿

- **æœ€é«˜å™ªå£°é²æ£’æ€§**: é˜ˆå€¼={thresholds[max_noise_idx]:.2f}æ—¶, Noise Robustness={noise_scores[max_noise_idx]:.3f}
- **è¶‹åŠ¿åˆ†æ**: {"é˜ˆå€¼è¶Šä½ï¼ŒæŠ—å¹»è§‰æ€§èƒ½è¶Šå¥½" if noise_scores[0] > noise_scores[-1] else "é˜ˆå€¼è¶Šé«˜ï¼ŒæŠ—å¹»è§‰æ€§èƒ½è¶Šå¥½"}

### 3. æƒè¡¡ç‚¹åˆ†æ

æœ€ä¼˜é˜ˆå€¼ **{optimal_threshold:.2f}** åœ¨å¬å›ç‡å’ŒæŠ—å¹»è§‰æ€§èƒ½ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡ï¼š

"""
    
    # ä¸æç«¯å€¼å¯¹æ¯”
    lowest_threshold_result = all_results[0]
    highest_threshold_result = all_results[-1]
    
    report += f"""- ç›¸æ¯”æœ€ä½é˜ˆå€¼({lowest_threshold_result['threshold']:.2f}):
  - Recall@3 å˜åŒ–: {best_result['avg_recall@3'] - lowest_threshold_result['avg_recall@3']:+.3f}
  - å™ªå£°é²æ£’æ€§å˜åŒ–: {best_result['noise_robustness'] - lowest_threshold_result['noise_robustness']:+.3f}

- ç›¸æ¯”æœ€é«˜é˜ˆå€¼({highest_threshold_result['threshold']:.2f}):
  - Recall@3 å˜åŒ–: {best_result['avg_recall@3'] - highest_threshold_result['avg_recall@3']:+.3f}
  - å™ªå£°é²æ£’æ€§å˜åŒ–: {best_result['noise_robustness'] - highest_threshold_result['noise_robustness']:+.3f}

"""
    
    report += "\n---\n\n## ğŸ’¡ å®æ–½å»ºè®®\n\n"
    
    report += f"""### æ¨èé…ç½®

```python
# åœ¨retrieval_mode_4_hybrid_with_rerankä¸­ä½¿ç”¨
OPTIMAL_THRESHOLD = {optimal_threshold:.2f}
```

### åœºæ™¯åŒ–å»ºè®®

1. **æ ‡å‡†åœºæ™¯ï¼ˆæ¨èï¼‰**
   - ä½¿ç”¨é˜ˆå€¼: **{optimal_threshold:.2f}**
   - é€‚ç”¨äº: éœ€è¦å¹³è¡¡å¬å›ç‡å’ŒæŠ—å¹»è§‰æ€§èƒ½çš„é€šç”¨åœºæ™¯
   - é¢„æœŸè¡¨ç°: Recall@3={best_result['avg_recall@3']:.3f}, å™ªå£°é²æ£’æ€§={best_result['noise_robustness']:.3f}

"""
    
    # æ‰¾å‡ºé«˜å¬å›ç‡åœºæ™¯çš„é˜ˆå€¼
    high_recall_result = max(all_results, key=lambda x: x['avg_recall@3'])
    high_noise_result = max(all_results, key=lambda x: x['noise_robustness'])
    
    report += f"""2. **é«˜å¬å›åœºæ™¯**
   - ä½¿ç”¨é˜ˆå€¼: **{high_recall_result['threshold']:.2f}**
   - é€‚ç”¨äº: éœ€è¦æœ€å¤§åŒ–å¬å›ç‡çš„åœºæ™¯ï¼ˆä¾‹å¦‚FAQç³»ç»Ÿï¼‰
   - é¢„æœŸè¡¨ç°: Recall@3={high_recall_result['avg_recall@3']:.3f}, å™ªå£°é²æ£’æ€§={high_recall_result['noise_robustness']:.3f}

3. **é«˜ç²¾åº¦åœºæ™¯**
   - ä½¿ç”¨é˜ˆå€¼: **{high_noise_result['threshold']:.2f}**
   - é€‚ç”¨äº: éœ€è¦æœ€å°åŒ–å¹»è§‰é£é™©çš„åœºæ™¯ï¼ˆä¾‹å¦‚æ³•å¾‹å’¨è¯¢ï¼‰
   - é¢„æœŸè¡¨ç°: Recall@3={high_noise_result['avg_recall@3']:.3f}, å™ªå£°é²æ£’æ€§={high_noise_result['noise_robustness']:.3f}

### è°ƒä¼˜å»ºè®®

- å¦‚æœå‘ç°å¬å›ç‡ä¸è¶³ï¼Œå¯ä»¥é€‚å½“é™ä½é˜ˆå€¼ï¼ˆå»ºè®®èŒƒå›´: {optimal_threshold-0.1:.2f} ~ {optimal_threshold:.2f}ï¼‰
- å¦‚æœå‘ç°å¹»è§‰é—®é¢˜ä¸¥é‡ï¼Œå¯ä»¥é€‚å½“æé«˜é˜ˆå€¼ï¼ˆå»ºè®®èŒƒå›´: {optimal_threshold:.2f} ~ {optimal_threshold+0.1:.2f}ï¼‰
- å®šæœŸä½¿ç”¨çœŸå®ç”¨æˆ·æŸ¥è¯¢é‡æ–°è¯„ä¼°é˜ˆå€¼ï¼Œå»ºè®®æ¯æœˆæˆ–æ¯å­£åº¦æ‰§è¡Œä¸€æ¬¡ä¼˜åŒ–

"""
    
    report += "\n---\n\n## ğŸ“Š å¯è§†åŒ–å›¾è¡¨\n\n"
    report += "è¯¦ç»†çš„å¯è§†åŒ–åˆ†æå›¾è¡¨å·²ç”Ÿæˆï¼ŒåŒ…æ‹¬ï¼š\n\n"
    report += "1. `threshold_vs_recall.png` - é˜ˆå€¼å¯¹å¬å›ç‡çš„å½±å“\n"
    report += "2. `threshold_vs_noise_robustness.png` - é˜ˆå€¼å¯¹æŠ—å¹»è§‰æ€§èƒ½çš„å½±å“\n"
    report += "3. `threshold_tradeoff.png` - å¬å›ç‡ä¸æŠ—å¹»è§‰æ€§èƒ½çš„æƒè¡¡åˆ†æ\n"
    report += "4. `threshold_combined_score.png` - åŠ æƒç»¼åˆå¾—åˆ†æ›²çº¿\n"
    report += "5. `threshold_vs_rejection_rate.png` - é˜ˆå€¼å¯¹æŸ¥è¯¢æ‹’ç»ç‡çš„å½±å“\n"
    
    report += "\n---\n\n## ğŸ“ è¯¦ç»†æ•°æ®\n\n"
    
    for result in all_results:
        report += f"""### é˜ˆå€¼: {result['threshold']:.2f}

"""
        for k_val in k_values:
            report += f"- Recall@{k_val}: {result[f'avg_recall@{k_val}']:.3f}\n"
        report += f"""- å™ªå£°é²æ£’æ€§: {result['noise_robustness']:.3f}
- ç»¼åˆå¾—åˆ†: {result['combined_score']:.3f}
- æ‹’ç»ç‡: {result['rejection_rate']*100:.1f}%
- å¯å›ç­”æŸ¥è¯¢æ•°: {result['answerable_queries']}
- æ— ç­”æ¡ˆæŸ¥è¯¢æ•°: {result['unanswerable_queries']}
- æ‹’ç»æŸ¥è¯¢æ•°: {result['rejected_queries']}
- å¹³å‡æ£€ç´¢æ—¶é—´: {result['avg_retrieval_time']:.3f}ç§’

"""
    
    # å†™å…¥æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nâœ… ä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")


# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("=" * 80)
    print("TripGuard RAGç³»ç»Ÿ Reranké˜ˆå€¼ä¼˜åŒ–æµ‹è¯•")
    print("=" * 80)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data_path = Path(__file__).parent / "test_data.json"
    test_data = load_test_data(test_data_path)
    print(f"\nâœ“ å·²åŠ è½½ {len(test_data)} æ¡æµ‹è¯•æ•°æ®")
    
    # åˆå§‹åŒ–LLMåˆ¤æ–­å™¨
    print("\nâœ“ æ­£åœ¨åˆå§‹åŒ–LLMåˆ¤æ–­å™¨...")
    llm = get_llm_judge()
    print("âœ“ LLMåˆ¤æ–­å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # å®šä¹‰è¦è¯„ä¼°çš„kå€¼
    k_values = [3, 5, 10]
    print(f"\nâœ“ å°†è¯„ä¼°ä»¥ä¸‹kå€¼çš„RecallæŒ‡æ ‡: {k_values}")
    
    # å®šä¹‰è¦æµ‹è¯•çš„é˜ˆå€¼èŒƒå›´
    thresholds = [round(x * 0.1, 1) for x in range(1, 10)]  # 0.1 ~ 0.9
    print(f"\nâœ“ å°†æµ‹è¯•ä»¥ä¸‹é˜ˆå€¼: {thresholds}")
    
    # æƒé‡è®¾ç½®ï¼ˆå¯æ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´ï¼‰
    alpha = 0.6  # å¬å›ç‡æƒé‡60%ï¼ŒæŠ—å¹»è§‰æƒé‡40%
    print(f"\nâœ“ è¯„ä¼°æƒé‡: å¬å›ç‡={alpha:.0%}, æŠ—å¹»è§‰={1-alpha:.0%}")
    
    # æ‰§è¡Œé˜ˆå€¼ä¼˜åŒ–
    optimal_threshold, all_results = find_optimal_threshold(
        thresholds,
        test_data,
        llm,
        k_values=k_values,
        alpha=alpha
    )
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(__file__).parent / "threshold_optimization_results"
    output_dir.mkdir(exist_ok=True)
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    print("\n" + "="*80)
    print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print("="*80)
    plot_threshold_analysis(all_results, k_values, output_dir, alpha=alpha)
    
    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    report_path = output_dir / f"Threshold_Optimization_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    generate_optimization_report(all_results, k_values, report_path, alpha=alpha)
    
    print("\n" + "="*80)
    print("âœ… é˜ˆå€¼ä¼˜åŒ–æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ¯ æ¨èé˜ˆå€¼: {optimal_threshold:.2f}")
    print(f"ğŸ“Š æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    print("="*80)


if __name__ == "__main__":
    main()
